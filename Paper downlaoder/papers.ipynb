{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583946e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['key', 'title', 'year', 'month', 'day', 'journal', 'issn', 'volume',\n",
       "       'issue', 'pages', 'authors', 'url', 'language', 'publisher', 'location',\n",
       "       'abstract', 'notes', 'doi', 'keywords', 'pubmed_id', 'pmc_id',\n",
       "       'PDF files'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"All details.csv\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b47b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DOI entries:\n",
      "0    10.1007/978-981-15-2184-3_82     WE  - Confere...\n",
      "1                                                  NaN\n",
      "2    10.1007/978-3-030-34252-4_8     WE  - Conferen...\n",
      "3                           10.6310/jog.201812_13(4).5\n",
      "4                                                  NaN\n",
      "5                                     10.1002/esp.5144\n",
      "6                                 10.1061/41050(357)97\n",
      "7                            10.1504/IJCAT.2020.107429\n",
      "8    10.12989/gae.2016.10.3.315     WE  - Science C...\n",
      "9                       10.1016/j.oceaneng.2023.115331\n",
      "Name: doi, dtype: object\n",
      "\n",
      "DOI column info:\n",
      "Total entries: 357\n",
      "Non-null DOI entries: 251\n",
      "Null DOI entries: 106\n"
     ]
    }
   ],
   "source": [
    "# Examine the DOI column to understand the format\n",
    "print(\"Sample DOI entries:\")\n",
    "print(data['doi'].head(10))\n",
    "print(\"\\nDOI column info:\")\n",
    "print(f\"Total entries: {len(data)}\")\n",
    "print(f\"Non-null DOI entries: {data['doi'].notna().sum()}\")\n",
    "print(f\"Null DOI entries: {data['doi'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c599a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of original vs cleaned DOIs:\n",
      "================================================================================\n",
      "                                                                                   Original DOI                    Cleaned DOI\n",
      "10.1007/978-981-15-2184-3_82     WE  - Conference Proceedings Citation Index - Science (CPCI-S)   10.1007/978-981-15-2184-3_82\n",
      "                                                                                            NaN                            NaN\n",
      " 10.1007/978-3-030-34252-4_8     WE  - Conference Proceedings Citation Index - Science (CPCI-S)    10.1007/978-3-030-34252-4_8\n",
      "                                                                     10.6310/jog.201812_13(4).5     10.6310/jog.201812_13(4).5\n",
      "                                                                                            NaN                            NaN\n",
      "                                                                               10.1002/esp.5144               10.1002/esp.5144\n",
      "                                                                           10.1061/41050(357)97           10.1061/41050(357)97\n",
      "                                                                      10.1504/IJCAT.2020.107429      10.1504/IJCAT.2020.107429\n",
      "            10.12989/gae.2016.10.3.315     WE  - Science Citation Index Expanded (SCI-EXPANDED)     10.12989/gae.2016.10.3.315\n",
      "                                                                 10.1016/j.oceaneng.2023.115331 10.1016/j.oceaneng.2023.115331\n",
      "                       10.1139/T08-050     WE  - Science Citation Index Expanded (SCI-EXPANDED)                10.1139/T08-050\n",
      "                                                                      10.1007/s40098-018-0333-3      10.1007/s40098-018-0333-3\n",
      "                                                                                            NaN                            NaN\n",
      "                                                                                            NaN                            NaN\n",
      "                                                                   10.37308/DFIJnl.20200929.225   10.37308/DFIJnl.20200929.225\n",
      "\n",
      "Summary:\n",
      "Original DOI entries with spaces: 124\n",
      "Cleaned DOI entries: 251\n",
      "DOIs that start with '10.': 250\n"
     ]
    }
   ],
   "source": [
    "# Clean the DOI column by extracting only the DOI part (before first space)\n",
    "def clean_doi(doi_text):\n",
    "    \"\"\"Extract only the DOI from the text, removing extra information after spaces\"\"\"\n",
    "    if pd.isna(doi_text):\n",
    "        return doi_text  # Keep NaN values as they are\n",
    "    \n",
    "    # Convert to string and split by space, take only the first part (the actual DOI)\n",
    "    cleaned_doi = str(doi_text).split()[0]\n",
    "    \n",
    "    # Additional validation: DOI should start with \"10.\"\n",
    "    if cleaned_doi.startswith('10.'):\n",
    "        return cleaned_doi\n",
    "    else:\n",
    "        return doi_text  # Return original if it doesn't look like a proper DOI\n",
    "\n",
    "# Apply the cleaning function to create a new column\n",
    "data['doi_cleaned'] = data['doi'].apply(clean_doi)\n",
    "\n",
    "# Show comparison of original vs cleaned DOIs\n",
    "print(\"Comparison of original vs cleaned DOIs:\")\n",
    "print(\"=\"*80)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original DOI': data['doi'].head(15),\n",
    "    'Cleaned DOI': data['doi_cleaned'].head(15)\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Original DOI entries with spaces: {data['doi'].str.contains(' ', na=False).sum()}\")\n",
    "print(f\"Cleaned DOI entries: {data['doi_cleaned'].notna().sum()}\")\n",
    "print(f\"DOIs that start with '10.': {data['doi_cleaned'].str.startswith('10.', na=False).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6d4975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of DOI cleaning:\n",
      "================================================================================\n",
      "Found 124 entries with extra text after DOI\n",
      "\n",
      "Before and after examples:\n",
      "1. Original: 10.1007/978-981-15-2184-3_82     WE  - Conference Proceedings Citation Index - Science (CPCI-S)\n",
      "   Cleaned:  10.1007/978-981-15-2184-3_82\n",
      "\n",
      "2. Original: 10.1007/978-3-030-34252-4_8     WE  - Conference Proceedings Citation Index - Science (CPCI-S)\n",
      "   Cleaned:  10.1007/978-3-030-34252-4_8\n",
      "\n",
      "3. Original: 10.12989/gae.2016.10.3.315     WE  - Science Citation Index Expanded (SCI-EXPANDED)\n",
      "   Cleaned:  10.12989/gae.2016.10.3.315\n",
      "\n",
      "4. Original: 10.1139/T08-050     WE  - Science Citation Index Expanded (SCI-EXPANDED)\n",
      "   Cleaned:  10.1139/T08-050\n",
      "\n",
      "5. Original: 10.1520/GTJ20170217     WE  - Science Citation Index Expanded (SCI-EXPANDED)\n",
      "   Cleaned:  10.1520/GTJ20170217\n",
      "\n",
      "✅ DOI column has been cleaned successfully!\n",
      "📊 Statistics:\n",
      "   - Total entries: 357\n",
      "   - Entries with DOI: 251\n",
      "   - Valid DOIs (starting with '10.'): 250\n",
      "\n",
      "💾 Cleaned data saved as 'All details_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# Show specific examples of the cleaning\n",
    "print(\"Examples of DOI cleaning:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find entries that had extra text (spaces in original)\n",
    "entries_with_spaces = data[data['doi'].str.contains(' ', na=False)]\n",
    "print(f\"Found {len(entries_with_spaces)} entries with extra text after DOI\")\n",
    "print(\"\\nBefore and after examples:\")\n",
    "\n",
    "for i, (idx, row) in enumerate(entries_with_spaces.head(5).iterrows()):\n",
    "    print(f\"{i+1}. Original: {row['doi']}\")\n",
    "    print(f\"   Cleaned:  {row['doi_cleaned']}\")\n",
    "    print()\n",
    "\n",
    "# Replace the original doi column with the cleaned version\n",
    "data['doi'] = data['doi_cleaned']\n",
    "# Drop the temporary cleaned column\n",
    "data = data.drop('doi_cleaned', axis=1)\n",
    "\n",
    "print(\"✅ DOI column has been cleaned successfully!\")\n",
    "print(f\"📊 Statistics:\")\n",
    "print(f\"   - Total entries: {len(data)}\")\n",
    "print(f\"   - Entries with DOI: {data['doi'].notna().sum()}\")\n",
    "print(f\"   - Valid DOIs (starting with '10.'): {data['doi'].str.startswith('10.', na=False).sum()}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "data.to_csv(\"All details_cleaned.csv\", index=False)\n",
    "print(f\"\\n💾 Cleaned data saved as 'All details_cleaned.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd85cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "823e2682-ab7c-4025-9a0b-1ebf89e8a280",
       "rows": [
        [
         "0",
         "10.1007/978-981-15-2184-3_82"
        ],
        [
         "1",
         null
        ],
        [
         "2",
         "10.1007/978-3-030-34252-4_8"
        ],
        [
         "3",
         "10.6310/jog.201812_13(4).5"
        ],
        [
         "4",
         null
        ],
        [
         "5",
         "10.1002/esp.5144"
        ],
        [
         "6",
         "10.1061/41050(357)97"
        ],
        [
         "7",
         "10.1504/IJCAT.2020.107429"
        ],
        [
         "8",
         "10.12989/gae.2016.10.3.315"
        ],
        [
         "9",
         "10.1016/j.oceaneng.2023.115331"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "0      10.1007/978-981-15-2184-3_82\n",
       "1                               NaN\n",
       "2       10.1007/978-3-030-34252-4_8\n",
       "3        10.6310/jog.201812_13(4).5\n",
       "4                               NaN\n",
       "5                  10.1002/esp.5144\n",
       "6              10.1061/41050(357)97\n",
       "7         10.1504/IJCAT.2020.107429\n",
       "8        10.12989/gae.2016.10.3.315\n",
       "9    10.1016/j.oceaneng.2023.115331\n",
       "Name: doi, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"doi\"].head(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbb97b",
   "metadata": {},
   "source": [
    "# 📚 Bulk Paper Download System\n",
    "\n",
    "This section implements a comprehensive system to:\n",
    "1. Filter papers that have valid DOIs\n",
    "2. Attempt to download papers from multiple sources\n",
    "3. Rename files using paper titles from the dataset\n",
    "4. Track download status in the dataset\n",
    "5. Save everything in an organized folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f643005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing filename cleaning:\n",
      "Original: A study of heat transfer: effects on chemical processes\n",
      "Cleaned:  A_study_of_heat_transfer_effects_on_chemical_processes\n",
      "--------------------------------------------------\n",
      "Original: Optimization of reactor design (Part 1)\n",
      "Cleaned:  Optimization_of_reactor_design_Part_1\n",
      "--------------------------------------------------\n",
      "Original: Mass transfer in porous media - A review\n",
      "Cleaned:  Mass_transfer_in_porous_media_-_A_review\n",
      "--------------------------------------------------\n",
      "Original: None\n",
      "Cleaned:  Unknown_Title\n",
      "--------------------------------------------------\n",
      "Original: Very long title that needs to be truncated because it exceeds the maximum allowed length for filenames\n",
      "Cleaned:  Very_long_title_that_needs_to_be_truncated_because_it_exceeds_the_maximum_allowed_length_for_filenam\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for downloading\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper function to clean filename\n",
    "def clean_filename(title, max_length=100):\n",
    "    \"\"\"Clean paper title to create a valid filename\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return \"Unknown_Title\"\n",
    "    \n",
    "    # Remove invalid characters for Windows filenames\n",
    "    title = str(title)\n",
    "    title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
    "    title = re.sub(r'[^\\w\\s\\-.]', '', title)\n",
    "    title = re.sub(r'\\s+', '_', title.strip())\n",
    "    \n",
    "    # Limit length\n",
    "    if len(title) > max_length:\n",
    "        title = title[:max_length]\n",
    "    \n",
    "    return title if title else \"Unknown_Title\"\n",
    "\n",
    "# Test the filename cleaning function\n",
    "print(\"Testing filename cleaning:\")\n",
    "sample_titles = [\n",
    "    \"A study of heat transfer: effects on chemical processes\",\n",
    "    \"Optimization of reactor design (Part 1)\",\n",
    "    \"Mass transfer in porous media - A review\",\n",
    "    None,\n",
    "    \"Very long title that needs to be truncated because it exceeds the maximum allowed length for filenames\"\n",
    "]\n",
    "\n",
    "for title in sample_titles:\n",
    "    cleaned = clean_filename(title)\n",
    "    print(f\"Original: {title}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b60d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset for valid DOIs...\n",
      "Original dataset size: 357\n",
      "After removing NaN DOIs: 251\n",
      "After removing invalid DOI formats: 250\n",
      "\n",
      "✅ Filtered dataset ready with 250 papers to download\n",
      "Sample DOIs to download:\n",
      "  1. 10.1007/978-981-15-2184-3_82\n",
      "  2. 10.1007/978-3-030-34252-4_8\n",
      "  3. 10.6310/jog.201812_13(4).5\n",
      "  4. 10.1002/esp.5144\n",
      "  5. 10.1061/41050(357)97\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a copy of the dataset and filter out entries without DOIs\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Filter out entries where DOI is missing or invalid\n",
    "print(\"Filtering dataset for valid DOIs...\")\n",
    "print(f\"Original dataset size: {len(data_copy)}\")\n",
    "\n",
    "# Remove entries with NaN DOIs\n",
    "data_filtered = data_copy[data_copy['doi'].notna()].copy()\n",
    "print(f\"After removing NaN DOIs: {len(data_filtered)}\")\n",
    "\n",
    "# Remove entries that don't start with \"10.\" (invalid DOI format)\n",
    "data_filtered = data_filtered[data_filtered['doi'].str.startswith('10.', na=False)].copy()\n",
    "print(f\"After removing invalid DOI formats: {len(data_filtered)}\")\n",
    "\n",
    "# Add download status column\n",
    "data_filtered['downloaded'] = False\n",
    "data_filtered['download_filename'] = \"\"\n",
    "data_filtered['download_status'] = \"Not attempted\"\n",
    "\n",
    "print(f\"\\n✅ Filtered dataset ready with {len(data_filtered)} papers to download\")\n",
    "print(f\"Sample DOIs to download:\")\n",
    "for i, doi in enumerate(data_filtered['doi'].head(5)):\n",
    "    print(f\"  {i+1}. {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdca8154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing download function with first DOI...\n",
      "   Attempting to download: 10.1007/978-981-15-2184-3_82\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "Test result: Success=False, Filename=, Status=Download failed - no sources available\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Download functions with multiple sources\n",
    "\n",
    "def check_open_access(doi):\n",
    "    \"\"\"Check if paper is open access using Unpaywall API\"\"\"\n",
    "    try:\n",
    "        # Using a generic email - replace with your actual email for better results\n",
    "        url = f\"https://api.unpaywall.org/v2/{doi}?email=student@university.edu\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('is_oa', False):\n",
    "                oa_location = data.get('best_oa_location', {})\n",
    "                pdf_url = oa_location.get('url_for_pdf') or oa_location.get('host_type_set')\n",
    "                return True, pdf_url\n",
    "        \n",
    "        return False, None\n",
    "    except Exception as e:\n",
    "        print(f\"   Unpaywall API error: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "def download_from_scihub(doi):\n",
    "    \"\"\"Attempt to download from Sci-Hub (check legal implications in your jurisdiction)\"\"\"\n",
    "    scihub_urls = [\n",
    "        \"https://sci-hub.se/\",\n",
    "        \"https://sci-hub.st/\", \n",
    "        \"https://sci-hub.ru/\",\n",
    "        \"https://sci-hub.wf/\"\n",
    "    ]\n",
    "    \n",
    "    for base_url in scihub_urls:\n",
    "        try:\n",
    "            url = f\"{base_url}{doi}\"\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)\n",
    "            \n",
    "            # Check if we got a PDF response\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            if response.status_code == 200 and ('pdf' in content_type or len(response.content) > 10000):\n",
    "                return response.content\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue  # Try next URL\n",
    "    \n",
    "    return None\n",
    "\n",
    "def download_paper(doi, title, download_folder=\"downloaded_papers\"):\n",
    "    \"\"\"Download a single paper using multiple methods\"\"\"\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    print(f\"   Attempting to download: {doi}\")\n",
    "    \n",
    "    # Method 1: Check Open Access first\n",
    "    print(\"   Checking Open Access...\")\n",
    "    is_oa, oa_url = check_open_access(doi)\n",
    "    \n",
    "    if is_oa and oa_url:\n",
    "        try:\n",
    "            response = requests.get(oa_url, timeout=20)\n",
    "            if response.status_code == 200:\n",
    "                content = response.content\n",
    "                source = \"Open Access\"\n",
    "            else:\n",
    "                content = None\n",
    "        except:\n",
    "            content = None\n",
    "    else:\n",
    "        content = None\n",
    "    \n",
    "    # Method 2: Try Sci-Hub if Open Access failed\n",
    "    if content is None:\n",
    "        print(\"   Trying Sci-Hub...\")\n",
    "        content = download_from_scihub(doi)\n",
    "        source = \"Sci-Hub\"\n",
    "    \n",
    "    # Save the file if we got content\n",
    "    if content and len(content) > 1000:  # Minimum size check\n",
    "        # Create filename from title\n",
    "        clean_title = clean_filename(title, max_length=80)\n",
    "        filename = f\"{clean_title}.pdf\"\n",
    "        filepath = os.path.join(download_folder, filename)\n",
    "        \n",
    "        # Handle duplicate filenames\n",
    "        counter = 1\n",
    "        base_filepath = filepath\n",
    "        while os.path.exists(filepath):\n",
    "            name, ext = os.path.splitext(base_filepath)\n",
    "            filepath = f\"{name}_{counter}{ext}\"\n",
    "            counter += 1\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            print(f\"   ✅ Downloaded successfully from {source}\")\n",
    "            return True, os.path.basename(filepath), f\"Downloaded from {source}\"\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error saving file: {str(e)}\")\n",
    "            return False, \"\", f\"Error saving: {str(e)}\"\n",
    "    \n",
    "    print(f\"   ❌ Download failed\")\n",
    "    return False, \"\", \"Download failed - no sources available\"\n",
    "\n",
    "# Test download function with one paper\n",
    "print(\"Testing download function with first DOI...\")\n",
    "test_doi = data_filtered['doi'].iloc[0]\n",
    "test_title = data_filtered['title'].iloc[0] if 'title' in data_filtered.columns else \"Test Paper\"\n",
    "\n",
    "success, filename, status = download_paper(test_doi, test_title)\n",
    "print(f\"Test result: Success={success}, Filename={filename}, Status={status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a65433db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Download Configuration:\n",
      "Total papers available for download: 250\n",
      "\\nOptions:\n",
      "1. Download all papers (could take a long time)\n",
      "2. Download first 10 papers (for testing)\n",
      "3. Download first 50 papers\n",
      "4. Custom number\n",
      "\\n🔧 For demonstration, downloading first 5 papers...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Main bulk download function\n",
    "def bulk_download_papers(data_filtered, download_folder=\"downloaded_papers\", max_papers=None, delay=3):\n",
    "    \"\"\"\n",
    "    Bulk download papers from filtered dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - data_filtered: DataFrame with valid DOIs\n",
    "    - download_folder: Folder to save downloaded papers\n",
    "    - max_papers: Maximum number of papers to download (None for all)\n",
    "    - delay: Delay between downloads in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    # Limit papers if specified\n",
    "    if max_papers:\n",
    "        papers_to_download = data_filtered.head(max_papers).copy()\n",
    "    else:\n",
    "        papers_to_download = data_filtered.copy()\n",
    "    \n",
    "    print(f\"🚀 Starting bulk download of {len(papers_to_download)} papers...\")\n",
    "    print(f\"📁 Download folder: {download_folder}\")\n",
    "    print(f\"⏱️  Delay between downloads: {delay} seconds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    successful_downloads = 0\n",
    "    failed_downloads = 0\n",
    "    \n",
    "    for index, (idx, row) in enumerate(papers_to_download.iterrows(), 1):\n",
    "        doi = row['doi']\n",
    "        title = row.get('title', f\"Paper_{index}\")\n",
    "        \n",
    "        print(f\"\\n[{index}/{len(papers_to_download)}] Processing: {title[:60]}...\")\n",
    "        print(f\"DOI: {doi}\")\n",
    "        \n",
    "        # Attempt download\n",
    "        success, filename, status = download_paper(doi, title, download_folder)\n",
    "        \n",
    "        # Update the filtered dataset\n",
    "        data_filtered.loc[idx, 'downloaded'] = success\n",
    "        data_filtered.loc[idx, 'download_filename'] = filename\n",
    "        data_filtered.loc[idx, 'download_status'] = status\n",
    "        \n",
    "        if success:\n",
    "            successful_downloads += 1\n",
    "        else:\n",
    "            failed_downloads += 1\n",
    "        \n",
    "        # Progress update\n",
    "        print(f\"   Status: {status}\")\n",
    "        print(f\"   Progress: {successful_downloads} successful, {failed_downloads} failed\")\n",
    "        \n",
    "        # Add delay between downloads\n",
    "        if index < len(papers_to_download):\n",
    "            print(f\"   Waiting {delay} seconds before next download...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🎉 Bulk download completed!\")\n",
    "    print(f\"✅ Successful downloads: {successful_downloads}\")\n",
    "    print(f\"❌ Failed downloads: {failed_downloads}\")\n",
    "    print(f\"📊 Success rate: {(successful_downloads/(successful_downloads+failed_downloads)*100):.1f}%\")\n",
    "    \n",
    "    return data_filtered\n",
    "\n",
    "# Ask user for download preferences\n",
    "print(\"📋 Download Configuration:\")\n",
    "print(f\"Total papers available for download: {len(data_filtered)}\")\n",
    "print(\"\\\\nOptions:\")\n",
    "print(\"1. Download all papers (could take a long time)\")\n",
    "print(\"2. Download first 10 papers (for testing)\")\n",
    "print(\"3. Download first 50 papers\")\n",
    "print(\"4. Custom number\")\n",
    "\n",
    "# For demo purposes, let's start with a small number\n",
    "test_download_count = 5\n",
    "print(f\"\\\\n🔧 For demonstration, downloading first {test_download_count} papers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77c6f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting bulk download process...\n",
      "⚠️  Starting with a small test batch. Modify 'max_papers' parameter to download more.\n",
      "🚀 Starting bulk download of 10 papers...\n",
      "📁 Download folder: downloaded_papers\n",
      "⏱️  Delay between downloads: 3 seconds\n",
      "================================================================================\n",
      "\n",
      "[1/10] Processing: 1G laboratory-scale shaking table tests on reduction of liqu...\n",
      "DOI: 10.1007/978-981-15-2184-3_82\n",
      "   Attempting to download: 10.1007/978-981-15-2184-3_82\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 1 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[2/10] Processing: A Case Study on Buckling Stability of Piles in Liquefiable G...\n",
      "DOI: 10.1007/978-3-030-34252-4_8\n",
      "   Attempting to download: 10.1007/978-3-030-34252-4_8\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 2 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[3/10] Processing: A case study on seismic response analysis of ground improved...\n",
      "DOI: 10.6310/jog.201812_13(4).5\n",
      "   Attempting to download: 10.6310/jog.201812_13(4).5\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 3 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[4/10] Processing: A catastrophic flowslide that overrides a liquefied substrat...\n",
      "DOI: 10.1002/esp.5144\n",
      "   Attempting to download: 10.1002/esp.5144\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 4 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[5/10] Processing: A comparison study of engineering approaches for seismic eva...\n",
      "DOI: 10.1061/41050(357)97\n",
      "   Attempting to download: 10.1061/41050(357)97\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 5 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[6/10] Processing: A large-scale field test on sand compaction piles including ...\n",
      "DOI: 10.1504/IJCAT.2020.107429\n",
      "   Attempting to download: 10.1504/IJCAT.2020.107429\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 6 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[7/10] Processing: A new design chart for estimating friction angle between soi...\n",
      "DOI: 10.12989/gae.2016.10.3.315\n",
      "   Attempting to download: 10.12989/gae.2016.10.3.315\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 7 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[8/10] Processing: A novel numerical strategy to analyse the installation and s...\n",
      "DOI: 10.1016/j.oceaneng.2023.115331\n",
      "   Attempting to download: 10.1016/j.oceaneng.2023.115331\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 8 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[9/10] Processing: A numerical study into lateral cyclic nonlinear soil-pile re...\n",
      "DOI: 10.1139/T08-050\n",
      "   Attempting to download: 10.1139/T08-050\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 9 failed\n",
      "   Waiting 3 seconds before next download...\n",
      "\n",
      "[10/10] Processing: A Numerical Study of Granular Pile Anchors Subjected to Upli...\n",
      "DOI: 10.1007/s40098-018-0333-3\n",
      "   Attempting to download: 10.1007/s40098-018-0333-3\n",
      "   Checking Open Access...\n",
      "   Trying Sci-Hub...\n",
      "   ❌ Download failed\n",
      "   Status: Download failed - no sources available\n",
      "   Progress: 0 successful, 10 failed\n",
      "\n",
      "================================================================================\n",
      "🎉 Bulk download completed!\n",
      "✅ Successful downloads: 0\n",
      "❌ Failed downloads: 10\n",
      "📊 Success rate: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Execute bulk download (start with limited number for testing)\n",
    "print(\"🚀 Starting bulk download process...\")\n",
    "print(\"⚠️  Starting with a small test batch. Modify 'max_papers' parameter to download more.\")\n",
    "\n",
    "# Run bulk download with first 10 papers\n",
    "data_with_downloads = bulk_download_papers(\n",
    "    data_filtered, \n",
    "    download_folder=\"downloaded_papers\", \n",
    "    max_papers=10,  # Change this number or set to None for all papers\n",
    "    delay=3  # 3 seconds delay between downloads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the updated dataset with download status\n",
    "print(\"\\n📊 Download Results Summary:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show download statistics\n",
    "downloaded_count = data_with_downloads['downloaded'].sum()\n",
    "total_attempted = len(data_with_downloads)\n",
    "\n",
    "print(f\"📈 Download Statistics:\")\n",
    "print(f\"   - Total papers with valid DOIs: {total_attempted}\")\n",
    "print(f\"   - Successfully downloaded: {downloaded_count}\")\n",
    "print(f\"   - Failed downloads: {total_attempted - downloaded_count}\")\n",
    "print(f\"   - Success rate: {(downloaded_count/total_attempted*100):.1f}%\")\n",
    "\n",
    "# Show download status breakdown\n",
    "status_counts = data_with_downloads['download_status'].value_counts()\n",
    "print(f\"\\n📋 Download Status Breakdown:\")\n",
    "for status, count in status_counts.items():\n",
    "    print(f\"   - {status}: {count}\")\n",
    "\n",
    "# Save the dataset with download information\n",
    "data_with_downloads.to_csv(\"papers_with_download_status.csv\", index=False)\n",
    "print(f\"\\n💾 Updated dataset saved as 'papers_with_download_status.csv'\")\n",
    "\n",
    "# Show sample of downloaded papers\n",
    "downloaded_papers = data_with_downloads[data_with_downloads['downloaded'] == True]\n",
    "if len(downloaded_papers) > 0:\n",
    "    print(f\"\\n✅ Successfully Downloaded Papers:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (_, row) in enumerate(downloaded_papers.head(5).iterrows(), 1):\n",
    "        title = row.get('title', 'Unknown Title')\n",
    "        filename = row['download_filename']\n",
    "        print(f\"{i}. {title[:60]}...\")\n",
    "        print(f\"   File: {filename}\")\n",
    "        print(f\"   DOI: {row['doi']}\")\n",
    "        print()\n",
    "\n",
    "# Show folder structure\n",
    "print(f\"\\n📁 Downloaded papers are saved in the 'downloaded_papers' folder\")\n",
    "if os.path.exists(\"downloaded_papers\"):\n",
    "    files_in_folder = os.listdir(\"downloaded_papers\")\n",
    "    print(f\"   Total files in folder: {len(files_in_folder)}\")\n",
    "    for file in files_in_folder[:5]:  # Show first 5 files\n",
    "        print(f\"   - {file}\")\n",
    "    if len(files_in_folder) > 5:\n",
    "        print(f\"   ... and {len(files_in_folder) - 5} more files\")\n",
    "else:\n",
    "    print(\"   Folder not created yet (no successful downloads)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e016cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Configuration for downloading more papers\n",
    "print(\"🔧 Configuration for Bulk Download:\")\n",
    "print(\"=\"*80)\n",
    "print(\"To download more papers, modify and run the following cell:\")\n",
    "print()\n",
    "print(\"# Uncomment and modify the following lines to download more papers:\")\n",
    "print(\"# For all papers:\")\n",
    "print(\"# data_final = bulk_download_papers(data_filtered, max_papers=None, delay=5)\")\n",
    "print()\n",
    "print(\"# For specific number of papers:\")\n",
    "print(\"# data_final = bulk_download_papers(data_filtered, max_papers=50, delay=3)\")\n",
    "print()\n",
    "print(\"# For faster downloads (less delay, but be respectful to servers):\")\n",
    "print(\"# data_final = bulk_download_papers(data_filtered, max_papers=20, delay=2)\")\n",
    "\n",
    "print(f\"\\n📊 Current Status:\")\n",
    "print(f\"   - Papers available for download: {len(data_filtered)}\")\n",
    "print(f\"   - Papers already processed: {len(data_with_downloads)}\")\n",
    "print(f\"   - Successfully downloaded: {data_with_downloads['downloaded'].sum()}\")\n",
    "\n",
    "print(f\"\\n⚠️  Important Notes:\")\n",
    "print(\"   1. Downloading all papers may take several hours\")\n",
    "print(\"   2. Some downloads may fail due to paywalls or server issues\")\n",
    "print(\"   3. Be respectful with download delays to avoid overwhelming servers\")\n",
    "print(\"   4. Check legal implications of using Sci-Hub in your jurisdiction\")\n",
    "print(\"   5. Consider using your institutional access for better results\")\n",
    "\n",
    "# Example configuration for downloading more papers (commented out)\n",
    "\"\"\"\n",
    "# Uncomment the following to download all papers:\n",
    "data_final = bulk_download_papers(\n",
    "    data_filtered, \n",
    "    download_folder=\"downloaded_papers\", \n",
    "    max_papers=None,  # Set to None for all papers\n",
    "    delay=5  # Increase delay for more respectful downloading\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
